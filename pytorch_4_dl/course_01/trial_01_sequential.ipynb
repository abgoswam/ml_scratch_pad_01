{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "256960bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83600079",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManualModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(10, 20)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(20, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7421a1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_model = nn.Sequential(\n",
    "    nn.Linear(10, 20),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d6a3f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_model = ManualModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82609cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual output shape: torch.Size([3, 5])\n",
      "Sequential output shape: torch.Size([3, 5])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, 10)\n",
    "manual_out = manual_model(x)\n",
    "seq_out = seq_model(x)\n",
    "\n",
    "print(f\"Manual output shape: {manual_out.shape}\")\n",
    "print(f\"Sequential output shape: {seq_out.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fb9c6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does seq_model have a forward method? True\n",
      "\n",
      "What is seq_model? <class 'torch.nn.modules.container.Sequential'>\n"
     ]
    }
   ],
   "source": [
    "print(\"Does seq_model have a forward method?\", hasattr(seq_model, 'forward'))\n",
    "print(\"\\nWhat is seq_model?\", type(seq_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3071345c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Are they the same? True\n"
     ]
    }
   ],
   "source": [
    "# When you call seq_model(x), Python calls seq_model.forward(x) automatically\n",
    "# Let's verify they're the same:\n",
    "x = torch.randn(2, 10)\n",
    "\n",
    "output1 = seq_model(x)  # This calls __call__ which calls forward\n",
    "output2 = seq_model.forward(x)  # Direct call to forward\n",
    "\n",
    "print(\"\\nAre they the same?\", torch.allclose(output1, output2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "037831ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 10])\n"
     ]
    }
   ],
   "source": [
    "class ModelWithSkipConnection(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(10, 10)\n",
    "        self.layer2 = nn.Linear(10, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Can't do this with Sequential!\n",
    "        identity = x  # Save input\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = x + identity  # Add skip connection\n",
    "        return x\n",
    "\n",
    "# Try it\n",
    "model = ModelWithSkipConnection()\n",
    "x = torch.randn(2, 10)\n",
    "output = model(x)\n",
    "print(f\"Output shape: {output.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b507693e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10])\n"
     ]
    }
   ],
   "source": [
    "seq_model_2 = nn.Sequential(\n",
    "    nn.Linear(10, 10),\n",
    "    nn.Linear(10, 10)\n",
    ")\n",
    "\n",
    "seq_out_2 = seq_model_2(x) + x\n",
    "\n",
    "print(seq_out_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "639f184a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelWithSkipConnection(\n",
      "  (layer1): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (layer2): Linear(in_features=10, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d620b409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (1): Linear(in_features=10, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(seq_model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1306fe7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients work with outside skip: True\n",
      "Gradients work with inside skip: True\n",
      "\n",
      "See the problem? The skip connection is easy to forget!\n"
     ]
    }
   ],
   "source": [
    "# Test: Does autograd work with outside skip connection?\n",
    "seq_model_2 = nn.Sequential(\n",
    "    nn.Linear(10, 10),\n",
    "    nn.Linear(10, 10)\n",
    ")\n",
    "\n",
    "x = torch.randn(2, 10, requires_grad=True)\n",
    "\n",
    "# Both work with autograd!\n",
    "out1 = seq_model_2(x) + x  # Skip outside\n",
    "out2 = ModelWithSkipConnection()(x)  # Skip inside\n",
    "\n",
    "loss1 = out1.sum()\n",
    "loss2 = out2.sum()\n",
    "\n",
    "loss1.backward(retain_graph=True)\n",
    "print(\"Gradients work with outside skip:\", x.grad is not None)\n",
    "\n",
    "x.grad = None  # Reset\n",
    "loss2.backward()\n",
    "print(\"Gradients work with inside skip:\", x.grad is not None)\n",
    "\n",
    "# The REAL problem: Imagine using your model in a larger system\n",
    "class BiggerModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Can't use seq_model_2 cleanly here!\n",
    "        # We'd have to remember to add the skip in forward\n",
    "        self.feature_extractor = seq_model_2\n",
    "        self.classifier = nn.Linear(10, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Easy to forget the + x !\n",
    "        features = self.feature_extractor(x)  # Oops, missing + x\n",
    "        return self.classifier(features)\n",
    "\n",
    "print(\"\\nSee the problem? The skip connection is easy to forget!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c73c5309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=10, out_features=10, bias=True)\n",
       "  (1): Linear(in_features=10, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ab3ccab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before freezing:\n",
      "Layer 0 requires_grad: True\n",
      "Layer 1 requires_grad: True\n",
      "Layer 0 parameters:\n",
      "  0.weight: shape torch.Size([10, 10]), requires_grad=True\n",
      "  0.bias: shape torch.Size([10]), requires_grad=True\n",
      "  1.weight: shape torch.Size([10, 10]), requires_grad=True\n",
      "  1.bias: shape torch.Size([10]), requires_grad=True\n",
      "\n",
      "Freezing layer 0...\n",
      "\n",
      "After freezing:\n",
      "  0.weight: shape torch.Size([10, 10]), requires_grad=False\n",
      "  0.bias: shape torch.Size([10]), requires_grad=False\n",
      "  1.weight: shape torch.Size([10, 10]), requires_grad=False\n",
      "  1.bias: shape torch.Size([10]), requires_grad=False\n",
      "\n",
      "After freezing:\n",
      "Layer 0 requires_grad: False\n",
      "Layer 1 requires_grad: False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test: Does autograd work with outside skip connection?\n",
    "seq_model_3 = nn.Sequential(\n",
    "    nn.Linear(10, 10),\n",
    "    nn.Linear(10, 10)\n",
    ")\n",
    "\n",
    "print(\"Before freezing:\")\n",
    "print(f\"Layer 0 requires_grad: {seq_model_3[0].weight.requires_grad}\")\n",
    "print(f\"Layer 1 requires_grad: {seq_model_3[1].weight.requires_grad}\")\n",
    "\n",
    "print(\"Layer 0 parameters:\")\n",
    "for name, param in seq_model_3.named_parameters():\n",
    "    print(f\"  {name}: shape {param.shape}, requires_grad={param.requires_grad}\")\n",
    "\n",
    "print(\"\\nFreezing layer 0...\")\n",
    "for param in seq_model_3.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(\"\\nAfter freezing:\")\n",
    "for name, param in seq_model_3.named_parameters():\n",
    "    print(f\"  {name}: shape {param.shape}, requires_grad={param.requires_grad}\")\n",
    "\n",
    "print(\"\\nAfter freezing:\")\n",
    "print(f\"Layer 0 requires_grad: {seq_model_3[0].weight.requires_grad}\")\n",
    "print(f\"Layer 1 requires_grad: {seq_model_3[1].weight.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada4fb7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu_0710",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
