{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21bafa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3740bda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d154fe3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5985166",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transformation = transforms.Compose([transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b691c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.MNIST(root=\"./data\", \n",
    "                                           train=True,\n",
    "                                           download=True,\n",
    "                                           transform=data_transformation)\n",
    "val_dataset = torchvision.datasets.MNIST(root=\"./data\", \n",
    "                                         train=False,\n",
    "                                         download=True,\n",
    "                                         transform=data_transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bd7a9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 28\n",
    "num_channels = 1\n",
    "patch_size = 7\n",
    "num_patches = (img_size//patch_size)**2\n",
    "token_dim = 32\n",
    "num_heads = 4 \n",
    "transformer_blocks = 4  # L\n",
    "num_classes = 10\n",
    "batch_size=64\n",
    "mlp_hidden_dim = 64\n",
    "learning_rate = 3e-4\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7321b7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = dataloader.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = dataloader.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099558c1",
   "metadata": {},
   "source": [
    "# PART 1 : Patch Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e818092b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.patch_embed = nn.Conv2d(num_channels, token_dim, patch_size, patch_size)\n",
    "    \n",
    "    def forward(self, x):  # x: (64, 1, 28, 28)\n",
    "        x = self.patch_embed(x) # (batch, token_dim, patch_size, patch_size) = (64,32,4,4)\n",
    "        x = x.flatten(2)  # (64, 32, 16)\n",
    "        x = x.transpose(1, 2) # (64, 16, 32)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117dff28",
   "metadata": {},
   "source": [
    "# PART 2 : Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cea971e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layernorm1 = nn.LayerNorm(token_dim)\n",
    "        self.layernorm2 = nn.LayerNorm(token_dim)\n",
    "        self.multihead_attention = nn.MultiheadAttention(token_dim, num_heads, batch_first=True)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(token_dim, mlp_hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(mlp_hidden_dim, token_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual1 = x\n",
    "\n",
    "        x = self.layernorm1(x)\n",
    "        x = self.multihead_attention(x, x, x)[0] # context vector\n",
    "        x = x + residual1\n",
    "\n",
    "        residual2 = x\n",
    "        x = self.layernorm2(x)\n",
    "        x = self.mlp(x)\n",
    "        x = x + residual2\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd1c63a",
   "metadata": {},
   "source": [
    "# PART 3 : MLP Classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49ce64c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPHead(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layernorm = nn.LayerNorm(token_dim)\n",
    "        self.mlp = nn.Linear(token_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layernorm(x)\n",
    "        x = self.mlp(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61d7fe1",
   "metadata": {},
   "source": [
    "# PART 1, 2 3 combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe4002c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Visitiontransformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.patch_embedding = PatchEmbedding()\n",
    "        self.cls_token = nn.Parameter(torch.randn(1,1,token_dim))\n",
    "        self.position_embedding = nn.Parameter(torch.rand(1, num_patches+1, token_dim))\n",
    "        self.transformer_blocks = nn.Sequential(*[TransformerEncoder() for _ in range(transformer_blocks)])\n",
    "        self.mlp_head = MLPHead()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embedding(x)\n",
    "        B = x.shape[0]\n",
    "        cls_token = self.cls_token.expand(B, -1, -1) # (B, 1, token_dim)\n",
    "        x = torch.cat((cls_token, x), dim=1) # (B, num_patches+1, token_dim)\n",
    "        x = x + self.position_embedding # (B, num_patches+1, token_dim)\n",
    "\n",
    "        x = self.transformer_blocks(x) # (B, num_patches+1, token_dim)\n",
    "        x = x[:, 0]\n",
    "        x = self.mlp_head(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a230747",
   "metadata": {},
   "source": [
    "# device, model, optimizer, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2a88100",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Visitiontransformer().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2db07cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.6641\n",
      "Validation Accuracy: 92.26%\n",
      "Epoch [2/5], Loss: 0.2082\n",
      "Validation Accuracy: 95.03%\n",
      "Epoch [3/5], Loss: 0.1540\n",
      "Validation Accuracy: 95.71%\n",
      "Epoch [4/5], Loss: 0.1204\n",
      "Validation Accuracy: 95.95%\n",
      "Epoch [5/5], Loss: 0.1060\n",
      "Validation Accuracy: 96.31%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, labels in train_dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Validation Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "899614b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m total_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (images, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mtrain_loader\u001b[49m):\n\u001b[1;32m      9\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     11\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_epoch = 0\n",
    "    total_epoch = 0\n",
    "    print(f\"\\nEpoch {epoch+1}\")\n",
    "\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct = (preds == labels).sum().item()\n",
    "        accuracy = 100.0 * correct / labels.size(0)\n",
    "\n",
    "        correct_epoch += correct\n",
    "        total_epoch += labels.size(0)\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"  Batch {batch_idx+1:3d}: Loss = {loss.item():.4f}, Accuracy = {accuracy:.2f}%\")\n",
    "\n",
    "    epoch_acc = 100.0 * correct_epoch / total_epoch\n",
    "    print(f\"==> Epoch {epoch+1} Summary: Total Loss = {total_loss:.4f}, Accuracy = {epoch_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe089e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vision Transformer Class# Switch to evaluation mode\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "test_acc = 100.0 * correct / total\n",
    "print(f\"\\n==> Val Accuracy: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25664501",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Show 10 predictions from the first test batch\n",
    "model.eval()\n",
    "images, labels = next(iter(val_loader))\n",
    "images, labels = images.to(device), labels.to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model(images)\n",
    "    preds = outputs.argmax(dim=1)\n",
    "\n",
    "# Move to CPU for plotting\n",
    "images = images.cpu()\n",
    "preds = preds.cpu()\n",
    "labels = labels.cpu()\n",
    "\n",
    "# Plot first 10 images\n",
    "plt.figure(figsize=(12, 4))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(images[i].squeeze(), cmap='gray')\n",
    "    plt.title(f\"Pred: {preds[i].item()}\\nTrue: {labels[i].item()}\")\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu_0712",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
