{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "676db6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c541b08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EXPERIMENT 2: CONSTANT INITIALIZATION\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# EXPERIMENT 2: Constant Initialization - COMPLETE WORKING VERSION\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT 2: CONSTANT INITIALIZATION\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c20a2f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial W1:\n",
      "tensor([[0.5000, 0.5000, 0.5000],\n",
      "        [0.5000, 0.5000, 0.5000]], requires_grad=True)\n",
      "\n",
      "Notice: All 3 neurons have IDENTICAL weights [0.5, 0.5]\n"
     ]
    }
   ],
   "source": [
    "# Recreate input with requires_grad (or we can just use it without grad, that's fine)\n",
    "x = torch.tensor([[1.0, 2.0]])  # Input doesn't need grad\n",
    "\n",
    "# Create fresh tensors - IMPORTANT: All new\n",
    "W1_const = torch.ones(2, 3) * 0.5\n",
    "W1_const.requires_grad = True\n",
    "\n",
    "b1_const = torch.zeros(3)\n",
    "b1_const.requires_grad = True\n",
    "\n",
    "W2_const = torch.ones(3, 1) * 0.5\n",
    "W2_const.requires_grad = True\n",
    "\n",
    "b2_const = torch.zeros(1)\n",
    "b2_const.requires_grad = True\n",
    "\n",
    "print(\"\\nInitial W1:\")\n",
    "print(W1_const)\n",
    "print(\"\\nNotice: All 3 neurons have IDENTICAL weights [0.5, 0.5]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d928f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- FORWARD PASS ---\n",
      "Hidden pre-activation (x @ W1 + b1):\n",
      "tensor([[1.5000, 1.5000, 1.5000]], grad_fn=<AddBackward0>)\n",
      "Notice: All 3 values are IDENTICAL: 1.50\n",
      "\n",
      "Hidden activations (after relu/identity):\n",
      "tensor([[1.5000, 1.5000, 1.5000]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Output:\n",
      "tensor([[2.2500]], grad_fn=<AddBackward0>)\n",
      "Output value: 2.2500\n",
      "\n",
      "Target: 1.0\n",
      "Loss (MSE): 1.5625\n"
     ]
    }
   ],
   "source": [
    "# Forward pass - MANUAL (so we see everything)\n",
    "print(\"\\n--- FORWARD PASS ---\")\n",
    "z1 = x @ W1_const + b1_const  # Hidden layer pre-activation\n",
    "print(f\"Hidden pre-activation (x @ W1 + b1):\\n{z1}\")\n",
    "print(f\"Notice: All 3 values are IDENTICAL: {z1[0, 0].item():.2f}\")\n",
    "\n",
    "a1 = z1  # No activation for now (or use relu if you want)\n",
    "print(f\"\\nHidden activations (after relu/identity):\\n{a1}\")\n",
    "\n",
    "z2 = a1 @ W2_const + b2_const  # Output layer\n",
    "output = z2\n",
    "print(f\"\\nOutput:\\n{output}\")\n",
    "print(f\"Output value: {output.item():.4f}\")\n",
    "\n",
    "# Target and loss\n",
    "target = torch.tensor([[1.0]])\n",
    "print(f\"\\nTarget: {target.item()}\")\n",
    "\n",
    "loss = ((output - target) ** 2).mean()\n",
    "print(f\"Loss (MSE): {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf0408fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- BACKWARD PASS ---\n",
      "\n",
      "Gradients for W1 (∂L/∂W1):\n",
      "tensor([[1.2500, 1.2500, 1.2500],\n",
      "        [2.5000, 2.5000, 2.5000]])\n",
      "\n",
      "Row 0 (neuron 0 weights):  tensor([1.2500, 2.5000])\n",
      "Row 1 (neuron 1 weights):  tensor([1.2500, 2.5000])\n",
      "Row 2 (neuron 2 weights):  tensor([1.2500, 2.5000])\n",
      "\n",
      "Gradients for W2 (∂L/∂W2):\n",
      "tensor([[3.7500],\n",
      "        [3.7500],\n",
      "        [3.7500]])\n"
     ]
    }
   ],
   "source": [
    "# Backward pass\n",
    "print(\"\\n--- BACKWARD PASS ---\")\n",
    "loss.backward()\n",
    "\n",
    "print(\"\\nGradients for W1 (∂L/∂W1):\")\n",
    "print(W1_const.grad)\n",
    "print(\"\\nRow 0 (neuron 0 weights): \", W1_const.grad[:, 0])\n",
    "print(\"Row 1 (neuron 1 weights): \", W1_const.grad[:, 1])\n",
    "print(\"Row 2 (neuron 2 weights): \", W1_const.grad[:, 2])\n",
    "\n",
    "print(\"\\nGradients for W2 (∂L/∂W2):\")\n",
    "print(W2_const.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367ee87f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu_0710",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
